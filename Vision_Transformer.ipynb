{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juststudentinIT/Methods-of-Compression-and-Performance-Improvement-of-Deep-Neural-Networks/blob/main/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZIZb3Xy_EjY",
        "outputId": "e65ae19e-b6d5-4d45-bd47-b8d3ecb4c86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, losses, Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrJR-Q_v_eGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f6965f-a875-4154-ca66-57b4e554c9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR3um2XG_hH7",
        "outputId": "bc4b52ec-f919-459d-b263-88e2a59e47f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train: (60000, 28, 28) - y_train: (60000,)\n",
            "x_test: (10000, 28, 28) - y_test: (10000,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"x_train: {x_train.shape} - y_train: {y_train.shape}\")\n",
        "print(f\"x_test: {x_test.shape} - y_test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBM81N3p5Nr-"
      },
      "outputs": [],
      "source": [
        "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
        "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
        "\n",
        "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
        "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
        "\n",
        "x_train = tf.repeat(x_train, 3, axis=3)\n",
        "x_test = tf.repeat(x_test, 3, axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9ZeykJR_nWf"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "image_size = 72\n",
        "patch_size = 6  \n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [projection_dim * 2,projection_dim,] \n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]\n",
        "input_shape = (32,32,3)\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKBYvJbO_6rB",
        "outputId": "6c52a617-2c7c-40e0-85dd-9024882b7fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoPjDQ84_1NY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(72, 72),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqys_ytEAC6T"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox1RK1MbAIJW"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        " \n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yN0QnuCAKQI"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        " \n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'num_layers': self.num_layers,\n",
        "            'units': self.units,\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dropout': self.dropout,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-1ZzVyiBz3i"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape= input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        " \n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        " \n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZEqz2hYB2qo"
      },
      "outputs": [],
      "source": [
        "optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "model = create_vit_classifier()\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\n",
        "       keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYaSsm7hCIQI",
        "outputId": "f937a297-5b99-4fb7-cb19-51c1dd7ea18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "211/211 [==============================] - 3587s 17s/step - loss: 1.0684 - accuracy: 0.6661 - val_loss: 0.4900 - val_accuracy: 0.8285\n",
            "Epoch 2/5\n",
            "211/211 [==============================] - 3291s 16s/step - loss: 0.6258 - accuracy: 0.7703 - val_loss: 0.4237 - val_accuracy: 0.8457\n",
            "Epoch 3/5\n",
            "211/211 [==============================] - 3305s 16s/step - loss: 0.5440 - accuracy: 0.7994 - val_loss: 0.3797 - val_accuracy: 0.8558\n",
            "Epoch 4/5\n",
            "211/211 [==============================] - 3375s 16s/step - loss: 0.4940 - accuracy: 0.8168 - val_loss: 0.3520 - val_accuracy: 0.8670\n",
            "Epoch 5/5\n",
            "211/211 [==============================] - 3356s 16s/step - loss: 0.4639 - accuracy: 0.8326 - val_loss: 0.3360 - val_accuracy: 0.8733\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=5,\n",
        "    validation_split=0.1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB4wN3iR0JEl",
        "outputId": "279ba536-4d79-4919-abf8-f45e02e6c76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 215s 688ms/step - loss: 0.3451 - accuracy: 0.8713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3451457619667053, 0.8712999820709229]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.evaluate(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Post-training quantization"
      ],
      "metadata": {
        "id": "XAnvVTFJJY_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vypwI08TQtg4",
        "outputId": "12bb57f0-c350-47f8-bac7-c684d3f92561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 100). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpemo8vxwd/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpemo8vxwd/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS\n",
        "   # enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "#open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzGimF9G1rzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54988c4-d6a3-4c13-b023-2a91b5b048ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 100). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp4_nz0w0i/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp4_nz0w0i/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "#Model with quantizes weights 16 float\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] #DEFAULT Default optimization strategy that quantizes model weights.\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS\n",
        "   # enable TensorFlow ops.\n",
        "]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model_quant_16 = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79hxnuz12kyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35929da-d896-44c1-c639-41e3fdc60bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 100). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpknu8uetx/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpknu8uetx/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS_INT8, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS\n",
        "   ]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "tflite_model_quant_8 = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6wylDoB2mvG",
        "outputId": "53b192a6-f708-4cd4-d8b6-23340568e5b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42823456"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_16_file = tflite_models_dir/\"model_quant16f.tflite\"\n",
        "tflite_model_quant_16_file.write_bytes(bytes(tflite_model_quant_16))\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_8_file = tflite_models_dir/\"model_quant8u.tflite\"\n",
        "tflite_model_quant_8_file.write_bytes(bytes(tflite_model_quant_8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xRjfbLTTpZr"
      },
      "outputs": [],
      "source": [
        "# Helper function to run inference on a TFLite model\n",
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global x_test\n",
        "  x_test = x_test[-500:]\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file)) #Interpreter interface for running TensorFlow Lite models.\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = x_test[test_image_index]\n",
        "    test_label = y_test[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    #we want to avoid copying, so we  use the tensor() function to get a numpy buffer pointing\n",
        "    #to the input buffer in the tflite interpreter.\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image) \n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "    #print(i)\n",
        "\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgbufdl4T43w"
      },
      "outputs": [],
      "source": [
        "# Helper function to evaluate a TFLite model on all images\n",
        "import time\n",
        "def evaluate_model(tflite_file, model_type):\n",
        "  global x_test\n",
        "  global y_test\n",
        "  x_test = x_test[-500:]\n",
        "  y_test = y_test[-500:]\n",
        "\n",
        "  start_time = time.time()\n",
        "  test_image_indices = range(x_test.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "  #print(\"here pred\")\n",
        "  accuracy = (np.sum(y_test == predictions) * 100) / len(x_test)\n",
        "  spent_time = time.time() - start_time\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d) , time = %f' % (\n",
        "      model_type, accuracy, len(x_test), spent_time/len(x_test) ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8DutOkLRVDZ",
        "outputId": "a04a00d6-10f6-4fe0-b79f-688889981ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model in Mb: 162.79111099243164\n",
            "Quantized 16f model in Mb: 81.48712158203125\n",
            "Quantized 8u model in Mb: 40.839630126953125\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#print(\"Original_original model in Mb:\", os.path.getsize(head_model) / float(2**20))\n",
        "print(\"Original model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
        "print(\"Quantized 16f model in Mb:\", os.path.getsize(tflite_model_quant_16_file) / float(2**20))\n",
        "print(\"Quantized 8u model in Mb:\", os.path.getsize(tflite_model_quant_8_file) / float(2**20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkDdjtgYRWv_",
        "outputId": "e5705a98-908e-466c-c53e-6a300541e227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model accuracy is 89.0000% (Number of test samples=500) , time = 0.032268\n",
            "Quantized 16float model accuracy is 89.0000% (Number of test samples=500) , time = 0.034737\n",
            "Quantized 8uint model accuracy is 88.4000% (Number of test samples=500) , time = 0.652956\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_model_file, model_type=\"Original\")\n",
        "evaluate_model(tflite_model_quant_16_file, model_type=\"Quantized 16float\")\n",
        "evaluate_model(tflite_model_quant_8_file, model_type=\"Quantized 8uint\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Vision Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYI3xR0QK0nvQ563/QIf1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}